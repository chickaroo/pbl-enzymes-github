{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Code for Machine Learning with scikit-learn\n",
    "\n",
    "First, import all packages needed for your code. scikit-learn contains more or less all functions needed to prepare your dataset, train different ML models, and validate those using common performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define inputs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, PredefinedSplit\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As example, we use the iris dataset which is part of scikit-learn and a commonly used toy dataset for ML. It consists of 3 different types of flowers which are defined by length and width of sepal and petal (2 types of leaves in a flower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "# load data set (iris dataset)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "class_names = iris.target_names\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "print(iris.data[1:10])  # print first 10 rows of samples (each row = one sample, each column = one feature)\n",
    "print(iris.target)  # print labels we want to predict\n",
    "print(class_names)  # print names of classes (target vector is only numerical)\n",
    "print(feature_names)  # print feature names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with scikit-learn, the dataset needs to be converted into 2 arrays, one containing the input features (X) and one containing the labels (y). Both can only contain numerical values! For X, each row corresponds to a sample and each column to a feature. y is 1-dimensional and each row corresponds to the same sample as the corresponding row in X. The iris set is already in the correct format.\n",
    "\n",
    "Before training a ML model, the dataset should be split in 3 sets: Training, Cross-Training, and Testing. First, split off the test set. Please make sure to always use the same test set, also if you train different ML models. Usually, it helps to pre-define a train/test-split, save that and re-use it for every ML model you are training. Same is true for the cross-training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Whole set: 150\n",
      "Training Set: 120\n",
      "Test Set: 30\n"
     ]
    }
   ],
   "source": [
    "# Split the data into a training set and a test set or use your own pre-defined split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2)\n",
    "\n",
    "print(\"Dataset sizes:\\nWhole set: {}\\nTraining Set: {}\\nTest Set: {}\".\n",
    "      format(len(y), len(y_train), len(y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0. -1. -1. -1. -1. -1. -1. -1.  0. -1.  0.  0. -1. -1. -1.  0. -1. -1.\n",
      " -1. -1.  0. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  0. -1. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1. -1.\n",
      " -1.  0. -1.  0. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  0.  0. -1.  0.\n",
      "  0.  0. -1. -1. -1.  0. -1. -1.  0. -1. -1.  0. -1.  0. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  0. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1.  0. -1. -1. -1. -1.  0. -1.]\n"
     ]
    }
   ],
   "source": [
    "#making the test_fold array\n",
    "predefined_val = np.empty((120))\n",
    "predefined_val.fill(-1)\n",
    "\n",
    "random_values = random.sample(range(0,119), 24)\n",
    "for i in random_values:\n",
    "    predefined_val[i] = 0\n",
    "print(predefined_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array uses any unique non -1 integer as its own test set. Here's an explanation: https://stackoverflow.com/questions/43952230/predefinedsplit-function-in-sklearn\n",
    "Basically, if you set all your validation samples to 0 and all your training samples to -1, then you have manually split the data into validation and training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splilts:  1\n",
      "TRAIN: [  0   1   2   4   6   8   9  10  12  13  15  16  17  18  19  21  23  24\n",
      "  25  26  27  28  29  30  31  32  33  34  36  38  39  40  41  42  43  44\n",
      "  45  47  48  49  50  51  53  54  55  58  59  61  62  63  64  68  69  70\n",
      "  71  72  73  74  75  77  78  80  81  82  83  84  85  88  89  90  91  92\n",
      "  93  94  95  96  97  98  99 100 101 102 103 106 108 109 110 111 112 113\n",
      " 114 115 116 117 118 119]\n",
      "TEST: [  3   5   7  11  14  20  22  35  37  46  52  56  57  60  65  66  67  76\n",
      "  79  86  87 104 105 107]\n"
     ]
    }
   ],
   "source": [
    "print(\"splilts: \", ps.get_n_splits())\n",
    "for train_index, test_index in ps.split():\n",
    "    print(\"TRAIN:\", train_index)\n",
    "    print(\"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set is used for learning, e.g., the weights of an Artifical Neural Network. To optimize hyperparameters like number of hidden layers, hidden units or the learning rate, the cross-training set is used. The GridSearchCV of scikit-learn easily allows you to test a variety of parameter combination. \n",
    "\n",
    "You can use different scores to compare the models. As default, the accuracy is used.\n",
    "\n",
    "Usually, to increase the training set size, you do not validate on one specific subset of your training data, but perform cross-validation. You split your data in *k* splits. In the example below, we used StratifiedKFold to split the data in 5 splits. Then, 4 splits are used for training and one for validation and this procedure is repeated 5 times and each time another split is used for validation. This results in 5 different performance values, and usually, the model with the best average performance is picked as final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saman\\anaconda3\\envs\\python-intro\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\saman\\anaconda3\\envs\\python-intro\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0       0.058041           0.0         0.000000             0.0   \n",
      "1       0.076996           0.0         0.000000             0.0   \n",
      "2       0.057987           0.0         0.000000             0.0   \n",
      "3       0.082003           0.0         0.000000             0.0   \n",
      "4       0.090974           0.0         0.001004             0.0   \n",
      "5       0.088042           0.0         0.000989             0.0   \n",
      "\n",
      "  param_hidden_layer_sizes param_learning_rate_init  \\\n",
      "0                    (25,)                      0.1   \n",
      "1                    (25,)                     0.01   \n",
      "2                    (50,)                      0.1   \n",
      "3                    (50,)                     0.01   \n",
      "4                    (75,)                      0.1   \n",
      "5                    (75,)                     0.01   \n",
      "\n",
      "                                              params  split0_test_score  \\\n",
      "0  {'hidden_layer_sizes': (25,), 'learning_rate_i...           0.958333   \n",
      "1  {'hidden_layer_sizes': (25,), 'learning_rate_i...           0.958333   \n",
      "2  {'hidden_layer_sizes': (50,), 'learning_rate_i...           0.958333   \n",
      "3  {'hidden_layer_sizes': (50,), 'learning_rate_i...           0.958333   \n",
      "4  {'hidden_layer_sizes': (75,), 'learning_rate_i...           0.958333   \n",
      "5  {'hidden_layer_sizes': (75,), 'learning_rate_i...           0.958333   \n",
      "\n",
      "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
      "0         0.958333             0.0                1            0.979167   \n",
      "1         0.958333             0.0                1            0.979167   \n",
      "2         0.958333             0.0                1            0.979167   \n",
      "3         0.958333             0.0                1            0.979167   \n",
      "4         0.958333             0.0                1            0.989583   \n",
      "5         0.958333             0.0                1            1.000000   \n",
      "\n",
      "   mean_train_score  std_train_score  \n",
      "0          0.979167              0.0  \n",
      "1          0.979167              0.0  \n",
      "2          0.979167              0.0  \n",
      "3          0.979167              0.0  \n",
      "4          0.989583              0.0  \n",
      "5          1.000000              0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saman\\anaconda3\\envs\\python-intro\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation to optimize hyperparameters\n",
    "\n",
    "# Define cross-validation object\n",
    "# This uses pre-defined splits to ensure that you are always using the same splits\n",
    "\n",
    "#define your predefined split using the test_fold array created earlier\n",
    "ps = PredefinedSplit(test_fold = predefined_val)\n",
    "\n",
    "# Define predictor\n",
    "classifier = MLPClassifier(activation='relu', solver='adam', max_iter=200, early_stopping=False)\n",
    "\n",
    "# Define parameters we want to optimize and values we want to test\n",
    "# Here, we test different activation functions\n",
    "params = {'hidden_layer_sizes': [(25,), (50,), (75,)],\n",
    "          'learning_rate_init': [0.1, 0.01]}\n",
    "\n",
    "# Perform grid search\n",
    "#here, cv=ps to use predefines split as the \"cross validation\"\n",
    "grid = GridSearchCV(estimator=classifier, cv=ps, param_grid=params, \n",
    "                    return_train_score=True)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Analyse results\n",
    "\n",
    "cv_results = pd.DataFrame(grid.cv_results_)\n",
    "print(cv_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have chosen a final model, you apply it to the test set to obtain the final performance estimation. This is the performance you would publish. Never make any model decision on the test set! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(hidden_layer_sizes=(25,), learning_rate_init=0.1)\n",
      "[[11  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0  6]]\n",
      "Acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Use best estimator and assess performance on the test set\n",
    "\n",
    "# Calculate predictions\n",
    "best_classifier = grid.best_estimator_\n",
    "print(best_classifier)\n",
    "y_pred = best_classifier.predict(X_test)\n",
    "pred_score = best_classifier.score(X_test, y_test)\n",
    "\n",
    "# Calculate confusion matrix (showing tp, fp, tn, fn)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print('Acc: {}'.format(round(pred_score, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
